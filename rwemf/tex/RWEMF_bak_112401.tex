\documentclass[conference]{IEEEtran}
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
\hyphenation{op-tical net-works semi-conduc-tor}

\usepackage{amsmath}
\usepackage{algorithm}  
\usepackage{algorithmic}
\usepackage{threeparttable}
\usepackage{graphicx}

% \usepackage{changes}
%   \definechangesauthor[name={neo}, color=orange]{per}
%   \setremarkmarkup{(#2)}


\begin{document}

\title{A hybrid random-walk based web service recommendation enhanced by matrix factorization}

\author{
  \IEEEauthorblockN{Jun Li$^*$}
  \IEEEauthorblockA{School of Physics and \\Electronic Information Engineering, \\WenZhou University\\
  Email: omama@wzu.edu.cn}
  \and
  \IEEEauthorblockN{Jian Lin$^\dagger$}
  \IEEEauthorblockA{School of Physics and \\Electronic Information Engineering, \\WenZhou University\\
  % Email: neolinjian@gmail.com
  }
}

\maketitle

\begin{abstract}
Recently, the QoS(Qaulity of service) of Web Service that includes response-time, throughput and so on that needs more accuracy prediction. For many web service  callers, choosing the appropriate service in right time should be more significant events. So the web service recommendation is right to be the choice. The collaborative filtering is major approach to predict the QoS of more web service through the observed data. But the sparse density of data need new technology to enhance the accuracy of prediction. And the matrix factorization is also the common approach to solve the prediction. In this paper, we propose the new hybrid approach that combined the predictions with random-walk based and matrix factorization. Comprehensive experiments on the QoS data set of real-world web service, that our approach achieve the more accuracy predictions.
\end{abstract}

\begin{IEEEkeywords}
  random-work, web service recommendation, matrix factorization
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

%=========================================================================
\section{Introduction}
\par Overview the past few years, the collaborative filtering and matrix factorization have success in traditional fields of recommendation, such as Goods, Music, Movie and so on. The recommendation of web service was effected by the achievements. However, the scenario of web service is more complex that suffers from sparse data and incomplete related information. There are so many different web services distributing over heterogeneous network which contains several auto-systems. So the recommendation of web service should solve the problems that sparse QoS(Quality of Service) value collected from various with the intrusted information about location or network. In a word, more measures should be made to enhanced the limited information to achieve the more accuracy of web service recommendation. Only that, the system of web service can provide the more quality service.
\par Web Service QoS predicted information enhanced technology is developing fast. For example, time-aware recommendation that makes prediction by history call record, location-aware recommendation that make use of numbers of AS(auto system), IP or GPS(Global Position System). But the measures all achieve improvement in accuracy of prediction in small scale of the sparse data. Although the information is critical to prediction, the experiments prove the factor that the more appropriate neighborhood ranking can really boost the accuracy of prediction. So the paper that Random Walk Models can efficiently work in real-world dataset in the past few years. With the transition probability matrix which based on the principle of Markov random process, the no directed connected users can calculate the similarity in neighborhood selection.
\par In the field of web service recommendation, the random-walk model is efficient, but the accuracy of prediction needed more improvement. The matrix factorization had ever solved the sparse efficiently in similar scenario. Naturally, we will try combining the random-walk model with the matrix factorization to get the good performance. And the matrix factorization also is the best approach to reduction of dimensions, when we calculate the similarity between user and user, the time complexity will be smaller. With more high-efficiency model, the hybrid algorithm improves the accuracy of prediction in final.
\par  In summary, to solve the web service recommendation and to increase the accuracy of QoS prediction, in this paper, the contributions we made as following:
%=========================================================================
\begin{itemize}
\item We explore the sparse problem in known probability, and study the sparse dataset and statistic phenomenon in real-world through calculation.
\item We propose the hybrid approach to combine the user-based collaborative filtering with matrix factorization.
\item We conduct the experiments on real-world dataset, and achieve the best accuracy of QoS prediction.
\end{itemize}

%=========================================================================
\par The rest of this paper is organized as follows. Section \ref{S-RW} summarizes the related work and our thought about sparse dataset. Section \ref{S-HRWMF} introduces our approach to combine the CF and MF algorithm. Section \ref{S-EE} reports the experiments and analyst the result of approaches. Section \ref{S-CN} concludes the paper and discusses the future work. 

%=========================================================================
\section{Related Work}\label{S-RW}
In this section, we will introduce the intuition of sparse density data, and explore the sparse problem in ideal environment that the sampling rate given in advance. Then the reviews of recommendation model will be displayed, including collaborative filtering, matrix factorization, and random-walk model.

\subsection{Intuition of sparse density data}
\par In the real-world dataset environment, our recommendation system samples the whole dataset with $d \%$ density. Suppose that the matrix $Q$ have m users, n services, and the $Q\in \textbf{R}^{m\times n}$. $q_{ij}$ means the QoS value of $user_{i}$ called $service_{j}$. 
\par In our ideal sampling method that we suppose the sampled data following normal distribution. So the every user's sampling number of service is $n\times d \% $. Although the sampling method is ideal, the client user implements the sampling approach in the real environment easily. For example, with parameters $ m=300 , n=5000 , d\%=5\% $ , 300 users get about $ 300 \times 250 $ service QoS values from the dataset. We approximately calculate the expectation of number on the common invoked number of service. Firstly, we suppose the $user_i$ samples the 250 values in total, the $user_j (i \neq j)$ repeats the sampling process about 299 times. The model subordinates to the binomial distribution of $X \sim b(299,0.05)$, and we get the common invoked service number for $user_i$ by $Ex=14.95,D=14.20$. So the factor that samples with the sparse rate is hard to recover the data of whole. And the location-aware information improves the accuracy of collaborative filtering prediction merely, because the common invoked service scattering in different location area.

% \subsection{The extreme rate of data mining }
% \par \added{I do not know to express the subject that the }The extreme data mining rate maybe propose to solve our confusion that how the accuracy we can achieve by improve the recommended model. Maybe, we can use the statistic approaches to get the expectation about the extreme prediction. Finally, with the expectation we can improve our algorithm to achieve more accuracy efficiently.
% \par And the expectation of number about common invoked service is also the major latent factors in matrix factorization process. And Section \ref{S-EE} will displayed the relationship between the best latent factors parameter and the matrix factorization accuracy.

\subsection{The approach to solve the sparse density}
\par The sparse problem is always limited the recommendation, and also the hot topic study in recent years. The collaborative filtering is the simple model to give the efficient prediction. But with sparse data and large of empty value, the accuracy is hard to improve. The capital ideas including make use of the adherent information or enhance the connection between users. The front idea is limited by the discrete dataset. Another is efficient, when the connection enhanced by random walk graph. 
\par The matrix factorization is also the efficient approach through the low-rank matrix recovery. Although there are many mf-based approaches proposed in recent years, the main goal is to overcome the cold start problem and get the more precise predictions. In the service-recommendation, the combination of cf and mf model will achieve the more accuracy.      

\subsection{User-based Collaborative Filtering}
The CF(Collaborative Filtering)-based algorithm have been widely used. The CF mines a user's common invoked services, which is identified by response-time or throughput, by calculation of similarity (the Euclidean distance) between the $user_i$ and $user_j$.

\begin{equation}
dst(i,j)=\frac{1}{
  N_{ij}}\sqrt{\sum_{k \in S_{ik}}{(q_{ij}-q_{jk}})^{2}
} \label{eq1}
\end{equation}

where $S_{i,j}$ means the set includes the commonly invoked service by $user_{i}$ and $user_{j}$, and the $N_{ij}$ means the number of the common invoked service. By the Equation (\ref{eq1}), we get the distance between two users, there is a defect that the two users have no common invoked service, the distance will be zero, we identity the smaller value  means the more similarity between two users, so the condition should be excluded in algorithm. 

\begin{equation}
sim(i,j)=\frac{1}{
  1+\frac{1}{N_{ij}}\sqrt{\sum_{k \in S_{ij}}{(q_{ij}-q_{ij}})^{2}}
}  \label{eq2}
\end{equation}
where the number 1 in the denominator is a way of Laplacian smooth to avoid the denominator being 0. It can conclude that the pair of users with the smaller distance is calculated the value more near to number 1. In the reversed condition, the value will be number 0.

\par With the similarity calculated by front step. We can construct the similarity matrix $Sim \in \textbf{R}^{m\times m}$, the $s_{ij}$ means the similarity between $user_{i}$ and $user_{j}$. Then, we ranking the neighbors by cells of matrix, and choose the top K users to predict the QoS value with the Equation (\ref{eq3}).

\begin{equation}
q^{'}_{ik}=\frac{
  \sum_{j \in TopK_{i}}{Sim_{ij} \times (Q_{jk}-\overline{Q_{j}})}
  }{
  \sum_{j \in TopK_{i}}{Sim_{ij}}
}+\overline{Q_{i}} 
\label{eq3}
\end{equation}
where $\overline{Q_{j}}$ means the average value of $user_{j}$, the Equation (\ref{eq3}) also considers the different user has different baseline of QoS prediction. 

\subsection{Matrix Factorization}
The MF(Matrix Factorization) has also been chosen for its accuracy. By factorizing the matrix $Q\in \textbf{R}^{m \times n} $ into user and service latent matrix $U\in \textbf{R}^{m \times k}$, $S \in \textbf{R}^{n \times k}$. The decomposing process like  the Equation (\ref{eq4})(\ref{eq5})(\ref{eq6}).

\begin{equation}
\begin{aligned}
\mathop{\arg\min}_{U,S} \sum_{i=1}^{m}{\sum_{j=1}^{n}{
(Q_{ij}-U_{i} \cdot S_{j}^{T}})^{2}
} \\
+ \lambda_{U} \cdot \sum_{i=1}^{m}\|U_{i}\|_{F}^{2}
+ \lambda_{S} \cdot \sum_{i=1}^{n}\|S_{i}\|_{F}^{2}
\label{eq4}
\end{aligned}
\end{equation}

\begin{equation}
{\frac{dloss}{dU_{i}}={\sum_{j=1}^{n}{
(Q_{ij}-U_{i} \cdot S_{j}^{T}}) \cdot S_{j}
} 
+ \lambda_{U} \cdot \|U_{i}\|
\label{eq5}
} 
\end{equation}

\begin{equation}
\frac{dloss}{dS_{j}}={\sum_{i=1}^{m}{
(Q_{ij}-U_{i} \cdot S_{j}^{T}}) \cdot U_{i}
}
+ \lambda_{S} \cdot \|S_{j}\|
\label{eq6}
\end{equation}

where the Equation (\ref{eq4}) is used to minimize the loss of Equation, and the $\| \cdot \|_{F}$ denotes the Frobenius norm to penalize the norms of U and S. The Equation (\ref{eq5}) (\ref{eq6}), we can use the gradient descent algorithm with several iterations, and find appropriate matrix U and S at last. Finally, the QoS value will be predicted by the inner product of $U_{i} \cdot S_{j}$. 
\par However, the matrix factorization is independent process, the latent matrix $U \in \textbf{R}^{m \times k}$ can be used as dimension reduction matrix of origin matrix $Q \in \textbf{R}^{m \times n}$, the dimension reduces from n to k. To some degree, the condition that user is with sparse records will be alleviated, and the the data in large scale will be dealt efficiently in short time.

\subsection{Random-Walk model}
\par The random-walk that based on PageRank alike measures (a Markov process) to get more appropriate neighbors ranking with the transition matrix.
\par In the last two Subsection, we get the matrix $Sim \in \textbf{R}^{m \times m}$, the $Sim_{ij}$ means the similarity between $user_{i}$ and $user_{j}$ by one step. As we all known, if $user_{i}$ and $user_{j}$ are no direct connection with one step,
the $user_{i}$ is similar to $user_{k}$ with similarity $Sim_{ik}$, and the $user_{j}$ is similar to $user_{k}$ with similarity $Sim_{jk}$, then we can achieve the similarity between  $user_{j}$ is similar to $user_{k}$ by two step. 
\par So we build the graph $G(V_{U},Sim)$ and use the Markov chain to model the state transition of random walk. Let $U_{0} \in V_{U}$ and the $Sim_{0,k}$ means the similarity between $user_{0}$ and the others. There are two choices for $user_{0}$:
\begin{itemize}
\item the $user_{0}$ with $\alpha$ probability by one step to link to itself.
\item the $user_{0}$ with $1- \alpha$ probability by one step to link to others.
\end{itemize}

The process goes by following equation.
\begin{equation}
P(X_{t}=j|X_{0}=i)=\left\{\begin{array}{ll}
\alpha ^{t} \cdot M^{t-1} \cdot Sim_{i}^{T}&i \neq j,\\
\alpha ^{t} \cdot M^{t-1} \cdot Sim_{i}^{T} + 1-\alpha &i = j.
\end{array}\right.
\end{equation}
The t steps random walking is 
\begin{equation}
P_{t}=(1-\alpha)P_{0}+ \alpha M^{T}P_{t-1} 
\label{eqrw}
\end{equation}
In summary, we have the transition matrix M. Along with the step t being infinite, the probability will converge to be stable, which is decided by the steady state distribution of the Markov chain. When the probability is stable, then the $P_{t}$ will equal to $P_{t-1}$, then the Equation \ref{eqrw} can be further transformed into the following shape by linear algebra calculation.
\begin{equation}
P^{*}=(1-\alpha)(I-\alpha M^{T})^{-1}P_{0}
\label{eqrwfinal}
\end{equation} 
With the Equation \ref{eqrwfinal}, we can transform the matrix Sim more with more appropriate similarity value. And the $P_{0}$ is always initialized by identity matrix which means the user only cares its own similarity with others with probability 1.

%=========================================================================
\section{Hybrid approach with RW and MF}\label{S-HRWMF}
\par The matrix Q will be decomposed into U and S with  latent dimension k firstly. And the similarity matrix Sim will be calculated by $\frac{1}{k} \cdot U \cdot U^{T}$. Then the probabilistic matrix P achieved by extended Equation:
\begin{equation}
P_{i,j}=\frac{A_{ij} \times Sim_{ij}}{\sum_{k \in Adj_{i}}{A_{ik} \times Sim_{ik}}}
\end{equation}
The $AS_{ij}$ parameter refers to the location dataset. With the information of $user_{i}$ and $user_{j}$ including AS(auto system), country and no direct connection area, the $A_{ij}$ is set to 3,2,1 respectively. 
\par Through the Equation (\ref{eqrwfinal}), identity matrix $P_{0}$ and P, the final steady transition matrix $P^{*}$ will be achieved. The $P^{*}_{ij}$ means the similar probability between $user_{i}$ and $user_{j}$. And the enhanced weight that the parameter $\varphi_{i}$ can be easily calculated through Equation.
\begin{equation}
\varphi_{i}=\frac{1}{N(j)} \cdot \sum_{j}{\frac{Sim_{ij}}{P^{*}_{ij}}}
\end{equation}
\par At the end of random-walk stage, the Equation (\ref{eqsime}) calculates the revised similarity which affect the topK nearest neighbors selected eventually.
\begin{equation}\label{eqsime} 
Sim_{ij}^{*}=\frac
{
\varphi_{i} \times P^{*}_{ij} + \varphi_{j} \times P^{*}_{ji}
}{2}
\end{equation}
With the new similarity matrix $Sim^{*}$ value, the top K nearest neighbors will be selected. 
\begin{equation}
\begin{aligned}
q_{ij}^{*}= \lambda \cdot (
\frac{
  \sum_{j \in TopK_{i}}{Sim_{ij} \times (Q_{jk}-\overline{Q_{j}})}
}{\sum_{j \in TopK_{i}}{Sim_{ij}}
}+\overline{Q_{i}}
) \\ + (1-\lambda) \cdot \sum_{k}U_{ik} \cdot S_{jk}^{T}
\label{final}
\end{aligned}
\end{equation}
\par The final QoS prediction will be calculated by Equation (\ref{final}). With the parameter $\lambda$, the predictions can adjust to different scenarios. The CF algorithm in sparse data will be more low than the real QoS value, and the MF algorithm will be more low high that the real QoS value with the regularizations. The overfitting or underfitting and changed by combine algorithm. The details of algorithm is in Algorithm \ref{alg_RWEMF}. And the code of algorithm could been found in WebSite \footnote{github.com/neoinmatrix/neosci/tree/master/rwemf}.
\input{alg_rwemf}
%=========================================================================
\section{Experiment and Evaluation}\label{S-EE}
\subsection{Dataset and Description}
The dataset is from WS-DREAM \footnote{github.com/wsdream/wsdream-dataset}. The whole dataset includes two attribute sub-dataset: response time(RT) and throughput(TP). The statistics of dataset are shown in Table \ref{tb1}. The dataset reflects the real-world condition that we have few clients to observe the QoS value and there are so many service on the Internet. 
% table=============================================
\begin{table}[H]
\begin{threeparttable}
\caption{Statistics of dataset}
\label{tb1}
\begin{tabular}{c||c|c|c|c|c|c} 
\hline 
QoS & numU & numS & min & max & mean & std \\ 
\hline 
RT(sec) & 339   & 5825  & 0.001 & 19.999 & 0.9086 & 1.9727 \\ 
\hline 
TP(kbps) & 339   & 5825  & 0.004 & 1000  & 47.5617 & 110.7971  \\ 
\hline 
\end{tabular} 
\end{threeparttable}
\end{table}

\par The information about the location of users and services can get in Table \ref{tb2}. The row of ``user\_as'' means there are 339 users in the dataset. And the 339 users are distributing in 136 areas. Every area has at least 1 user and no more than 31 users. And the average of users on one area about 2.4745 with 2.8338 standard deviation. Notice that the postfix ``\_as" and ``\_ct" means area is as(auto system) and ct(country) respectively. From the statistic information about data, the fact that users or services distribute in different area are extremely unbalanced. The location dataset provides inefficiency information, that is why the location information is hardly enhanced the accuracy of our experiments.

% table=============================================
\begin{table}[H]
\begin{threeparttable}
\caption{Statistics of userinfo and serviceinfo}
\label{tb2}
\begin{tabular}{c||c|c||c|c|c|c}
\hline 
infoattr & num & size & min & max & mean & std \\
\hline
user\_as & 339   & 136   & 1     & 31    & 2.4745 & 2.8338 \\
\hline
user\_ct & 339   & 30    & 1     & 161   & 10.9355 & 28.3673 \\
\hline
service\_as & 5825  & 992   & 1     & 1212  & 5.8661 & 40.6092 \\
\hline
service\_ct & 5825  & 73    & 1     & 2395  & 78.7162 & 285.9846 \\
\hline
\end{tabular} 
\end{threeparttable}
\end{table}

\subsection{Evaluation Metric and Parameter}
The MAE(Mean Absolute Error) and NMAE(Normalized Mean Absolute Error) may be the common measurable metrics. MAE is defined as 
\begin{equation}
MAE=\frac{1}{N}\sum_{i,j}{|q_{ij}-q^{\hat{}}_{ij}|}
\end{equation}
The NMAE is computed as the MAE normalized by the mean of all values, which is defined as 
\begin{equation}
NMAE=\frac{MAE}{\frac{1}{N}\sum_{i,j}{|q_{ij}}|}
\end{equation}
The MAE reflects the absolute error of the predictions. The NMAE reflects the relative error of the predictions. We can compare the ability of predictions from different dataset with NMAE relatively.

\subsection{Result Accuracy and Comparison}
\par There are several classical recommendation algorithms in the experiments as the comparisons. The result of response time and throughput
 are displayed in Table \ref{tb_rt} and Table \ref{tb_tp} respectively.
\par The comparisons including
\begin{itemize}
\item UPCC is the user-based collaborative filtering algorithm that calculate the similarity between users with Pearson correlation coefficient. In this case of small number of users, the algorithm is fast with short running time. 
\item IPCC is the user-based collaborative filtering algorithm that calculate the similarity between users with Pearson correlation coefficient. In this case of large number of services, the algorithm is slow with long running time. 
\item UIPCC is the hybrid method linearly combines the results of UPCC and IPCC, but the accuracy is more precise than that two. With the running time of total two algorithm, the algorithm is also slow.
\item PMF is the matrix factorization algorithm with the model of probability. In this case with sparse data, the process is fast to be convergent to stable state. So the maximum iteration and convergent threshold are significant to keep the running time within acceptable range.
\item UL\_RWE is user-based random walk model enhanced by the matrix factorization. The reduced-dimension matrix U with k dimensions latent elements, the algorithm is more fast and achieve more accuracy.
\item RWEMF is our approach which are more efficient in the experiment. In the base of UL\_RWE, the approach successfully combined the matrix factorization prediction. The running time is close to matrix factorization to achieve more accuracy.
\end{itemize}

\begin{table}[H]
  \centering
  \caption{The MAE and NMAE of response time prediction}
  \label{tb_rt}
    \begin{tabular}{r||l|rrrr}
\hline
model & DS   & 5\%  & 10\% & 15\% & 20\% \\
\hline
UPCC & MAE  & 0.6159 & 0.5371 & 0.4966 & 0.4737 \\
         & NMAE & 0.6794 & 0.5917 & 0.5471 & 0.5219 \\
\hline
IPCC & MAE  & 0.6805 & 0.6572 & 0.5670 & 0.4955 \\
         & NMAE & 0.7507 & 0.7240 & 0.6246 & 0.5459 \\
\hline
UIPCC & MAE  & 0.6045 & 0.5336 & 0.4879 & 0.4601 \\
         & NMAE & 0.6668 & 0.5879 & 0.5374 & 0.5068 \\
\hline
PMF & MAE  & 0.5704 & 0.4894 & 0.4584 & 0.4390 \\
         & NMAE & 0.6292 & 0.5391 & 0.5050 & 0.4837 \\
\hline
UL-RWE & MAE  & 0.5255 & 0.4735 & 0.4462 & 0.4291 \\
         & NMAE & 0.5797 & 0.5216 & 0.4916 & 0.4727 \\
\hline
XEMF & MAE  & 0.5518 & 0.4891 & 0.4756 & 0.4877 \\
         & NMAE & 0.6087 & 0.5388 & 0.5239 & 0.5373 \\
\hline
RWEMF & MAE  & 0.5068 & 0.4560 & 0.4344 & 0.4251 \\
         & NMAE & 0.5591 & 0.5023 & 0.4786 & 0.4683 \\
\hline
    \end{tabular}
\end{table}


\begin{table}[H]
  \centering
  \caption{The MAE and NMAE of throughput prediction}
  \label{tb_tp}
    \begin{tabular}{r||l|rrrr}
\hline
model & DS & 5\% & 10\% & 15\% & 20\% \\
\hline
UPCC & MAE & 26.8039 & 22.2826 & 20.0274 & 18.689 \\
   & NMAE & 0.5643 & 0.4688 & 0.4212 & 0.3931 \\
\hline
IPCC & MAE & 29.5539 & 29.4531 & 30.1322 & 27.5450 \\
   & NMAE & 0.6222 & 0.6196 & 0.6338 & 0.5794 \\
\hline
UIPCC & MAE & 26.0401 & 21.9952 & 20.0911 & 18.6256 \\
   & NMAE & 0.5483 & 0.4627 & 0.4226 & 0.3918 \\
\hline
PMF & MAE & 22.5499 & 17.9761 & 16.5358 & 15.0594 \\
   & NMAE & 0.4748 & 0.3782 & 0.3478 & 0.3168 \\
\hline
UL-RWE & MAE & 19.4043 & 15.6509 & 14.3058 & 13.5797 \\
   & NMAE & 0.4085 & 0.3293 & 0.3009 & 0.2857 \\
\hline
XEMF & MAE & 21.0512 & 17.2567 & 15.9693 & 15.5798 \\
   & NMAE & 0.4432 & 0.3630 & 0.3359 & 0.3277 \\
\hline
RWEMF & MAE & 18.5121 & 15.1752 & 13.9855 & 13.3388 \\
   & NMAE & 0.3898 & 0.3193 & 0.2942 & 0.2806 \\
\hline
    \end{tabular}
\end{table}

\par Form the experimental results are shown in Table \ref{tb_rt} \ref{tb_tp}, we have some observations. 

\begin{itemize}
\item The matrix factorization algorithm(PMF) achieved more accuracy than the user-based or item-based without enhanced algorithm(UPCC,IPCC,UIPCC). 
\item The algorithm (HL-RWE) enhanced by random-walk model is achieve more accuracy than the similarity calculated based collaborative filtering algorithm(UPCC,IPCC,UIPCC). So the precision similarity calculation and the appropiate and ranking neighbors selected are the efficient approaches to improve the accuracy.
\item The RWEMF algorithm is more efficient than other algorithms and achieves the best accuracy. The sparse density of 5\% is more appropiate for the algorithm to have accuracy that the dense density of 20\%.
\item In the different sub-dataset, the algorithms achieve different performance. The response-time dataset with value range (0.001-19.999) and standard deviation 1.9727 is with fluctuation about 9.86\%. The throughput dataset with value range (0.004-1000) and standard deviation 110.797 is with fluctuation about 11.08\%. The RWEMF achieves $\frac{0.6794-0.5591}{0.6794}=0.1771$ in rt dataset and $\frac{0.5643-0.3898}{0.5643}=0.3092$ in tp dataset. So the sparse density and the fluctuation in the dataset is the important elements to the RWEMF algorithm.
\end{itemize}

\subsection{Analysis and Deduction}
\par The significant parameters in RWEMF are top K, latent dimensions, the rate of mf union. 

\par From the Figure \ref{fig_rt} \ref{fig_tp}, the number of nearby neighbors selected obviously effects the accuracy. Although the accuracy tendency is different in different dataset, the appropiate number of nearby neighbors selected decided the best accuracy in different sparse density. When topK=3, the RWEMF achieves the best accuracy in both response-time and throughput dataset. 

\begin{figure}[H]  
\centering  
\includegraphics[width=0.45\paperwidth]{topk_rt.eps}  
\caption{the MAE of different topK of RWEMF on the response-time dataset }  
\label{fig_rt}  
\end{figure} 

\begin{figure}[H] 
\centering  
\includegraphics[width=0.45\paperwidth]{topk_tp.eps}  
\caption{the MAE of different topK of RWEMF on the throughput dataset }  
\label{fig_tp}  
\end{figure} 



\par From the Figure \ref{fig_rumf_rt} \ref{fig_rumf_tp}, the rumf parameter is the rate united the mf. In the experiments, we choose the 5\% and 20\% density. And every rate of density has three lines including the rwe line ,mf line and rwemf line. It is clearly to know, the MAE of mf is largest in three, the MAE of rwe is smaller than mf's, with the 0.7 of rumf, the rwemf reaches the best accuracy of MAE. The phenomenon in the throughput dataset is similar. But the response-time dataset with small value is more sensitive to the rate, and it reaches the best accuracy in short range. 


\begin{figure}[H] 
\centering  
\includegraphics[width=0.45\paperwidth]{rumf_rt.eps}  
\caption{the MAE of different rate union mf on the response-time dataset }  
\label{fig_rumf_rt}  
\end{figure} 

\begin{figure}[H] 
\centering  
\includegraphics[width=0.45\paperwidth]{rumf_tp.eps}  
\caption{the MAE of different rate union mf  on the throughput dataset }  
\label{fig_rumf_tp}  
\end{figure} 

\par Every point in Figure \ref{fig_ae_rt} \ref{fig_ae_tp} means the prediction of RWEMF,RWE,MF three algorithm minus the real QoS value of dataset, and the points in view are sampling randomly that on behalf of the whole predictions. It is easy to see the AE(Absolute Error) of RWEMF is locating in the middle between RWE's and MF's. Sometimes, the RWE can get the accuracy, but it also processes with the big variance. And the ME can not get the accuracy, but it also runs steadily with the small variance. And the predictions of algorithms are sensitive to value of dataset. The absolute error in throughput dataset fluctuated in large range compared to response-time dataset's.

\begin{figure}[H] 
\centering  
\includegraphics[width=0.45\paperwidth]{ae_rt.eps}  
\caption{the Absolute Error on the response-time dataset }  
\label{fig_ae_rt}  
\end{figure} 

\begin{figure}[H] 
\centering  
\includegraphics[width=0.45\paperwidth]{ae_tp.eps}  
\caption{the Absolute Error on the throughput dataset }  
\label{fig_ae_tp}  
\end{figure} 

%=========================================================================
\section{Conclusion}\label{S-CN}
\par We propose the RWEMF a hybrid approach to best accuracy in QoS real-world web service dataset. Firstly, We explore the sparse density dataset through statistic calculation. Clearly, the similar calculation and the nearby neighbors selection are significant. And the combination of random-walk user-based collaborative filtering and matrix factorization algorithm is described in the papers. The experiments of RWEMF prove our algorithm is most efficient and the best parameters chosen made RWEMF achieved the best accuracy in this QoS dataset.
\par In the future, with the best accuracy in this dataset, the RWEMF can be extended by more efficient model. The short running time and exquisite mind can help the algorithm using in real-world web service recommendation easily. The parameters for the hybrid model need more exploration and more study to keep the algorithm more efficient.Although the adherent users's and service's information improve the accuracy finitely, there are more latent information value should be mined in the dataset. The MAE in 5\% on response-time dataset is 0.5068 now, Although the value is relative, sometime it could be metrics to measure the ability of algorithm in sparse dataset. Further, the MAE could be lower that 0.5000 by the new hybrid model.

%=========================================================================
\section*{Acknowledgment}
The authors would like to thank...
 \cite{liu_location-aware_2016} 
 \cite{yin_network_2017} 
 \cite{yu_personalized_2014} 
 \cite{ma_predicting_2015} 
 \cite{zheng_wsrec:_2009} 
 \cite{zheng_qos-aware_2011} 
 \cite{park_comparative_2017} 
 \cite{rodriguez-mier_hybrid_2017} 
 \cite{choi_statistical_2015} 
 \cite{agarwal_stochastic_2014} 
 \cite{zhou_qos-aware_2015} 
 \cite{xia_domain-aware_2014} 
 \cite{wang_collaborative_2015} 
 \cite{liu_incorporating_2015} 
 \cite{tang_location-aware_2012} 
 \cite{lo_efficient_2014} 
 \cite{chen_user_2016} 
 \cite{r_salakhutdinov__a_mnih_probability_2007} 
 \cite{hadad_tqos:_2010} 
 \cite{mohammed_markov_2016} 
 \cite{tran_low-rank_2016} 
 \cite{ongie_fast_2017} 
 \cite{koren_matrix_2009}.

\bibliographystyle{IEEEtran}
\bibliography{IEEEabrv,rwemfbib}

% \begin{thebibliography}{1}
% \input{rwemfbib.bib}
% % \bibitem{IEEEhowto:kopka}
% % H.~Kopka and P.~W. Daly, \emph{A Guide to \LaTeX}, 3rd~ed.\hskip 1em plus
% %   0.5em minus 0.4em\relax Harlow, England: Addison-Wesley, 1999.

% \end{thebibliography}

\end{document}

